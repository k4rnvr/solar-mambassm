{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DOWNLOAD AND UNZIP PARQUET FILES \n",
        "!pip install -q gdown\n",
        "!gdown \"https://drive.google.com/file/d/1JvN0-bv7frRf8BAxVrmtJoYV-vzjjt0d/view?usp=sharing\"\n",
        "!unzip -q solar_data.zip -d solar_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPrkkI7xWpKL"
      },
      "outputs": [],
      "source": [
        "#LOAD THE PARQUET FILES\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "parquet_files = sorted(glob.glob(\"solar_data/*.parquet\"))\n",
        "\n",
        "df_all = pd.concat([pd.read_parquet(file) for file in parquet_files], ignore_index=True)\n",
        "\n",
        "print(f\"Loaded combined DataFrame with shape: {df_all.shape}\")\n",
        "print(df_all['TIMESTAMP'].min(), \"â†’\", df_all['TIMESTAMP'].max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaTrm66OX8QZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = df_all.copy()\n",
        "\n",
        "df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'], errors='coerce')\n",
        "\n",
        "df.set_index('TIMESTAMP', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KhWLgY1ZEoc"
      },
      "outputs": [],
      "source": [
        "missing_summary = df.isna().sum().sort_values(ascending=False)\n",
        "print(missing_summary[missing_summary > 0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GRr6Ef2aFH4"
      },
      "outputs": [],
      "source": [
        "#DROPPING WINDREFVMIN AND INTERPOLATING DATA FOR TIME SERIES\n",
        "df.drop(columns=['WindRef_V_Min'], inplace=True)\n",
        "df = df.interpolate(method='time')  # best for time series\n",
        "df = df.fillna(method='bfill').fillna(method='ffill')  # fill edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYJkU9doajCH"
      },
      "outputs": [],
      "source": [
        "print(\"Remaining NaNs:\", df.isna().sum().sum())\n",
        "df.describe().T\n",
        "print(df.columns.tolist())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pube3WFky7jc"
      },
      "outputs": [],
      "source": [
        "print(df.index)\n",
        "import pandas as pd\n",
        "df['RECORD'] = pd.to_datetime(df['RECORD'], errors='coerce')\n",
        "\n",
        "#Check if any conversions failed\n",
        "print(df[df['RECORD'].isna()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rms0Oy_X3Ju9"
      },
      "outputs": [],
      "source": [
        "num_duplicates = df.index.duplicated().sum()\n",
        "print(f\"Number of duplicate timestamp entries: {num_duplicates}\")\n",
        "\n",
        "timestamp_counts = df.index.value_counts()\n",
        "print(\"Timestamps with counts > 1:\")\n",
        "print(timestamp_counts[timestamp_counts > 1])\n",
        "\n",
        "# Show all rows that share a timestamp with another row, grouped together\n",
        "duplicate_rows = df[df.index.duplicated(keep=False)]\n",
        "print(\"Comparing original and duplicate rows:\")\n",
        "print(duplicate_rows.sort_index().head(20)) # Print the first 20 rows to see a few pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l4RubpK49CK"
      },
      "outputs": [],
      "source": [
        "print(f\"Original DataFrame length: {len(df)}\")\n",
        "\n",
        "# REMOVE DUPLICATES\n",
        "df_clean = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "print(f\"Cleaned DataFrame length: {len(df_clean)}\")\n",
        "\n",
        "# VERIFY UNIQUENESS\n",
        "is_unique = df_clean.index.is_unique\n",
        "print(f\"Is the cleaned index unique? {is_unique}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5R0ZetB1kKm"
      },
      "outputs": [],
      "source": [
        "# Extract components from the TIMESTAMP\n",
        "df_clean['year'] = df_clean.index.year\n",
        "df_clean['month'] = df_clean.index.month\n",
        "df_clean['day'] = df_clean.index.day\n",
        "df_clean['hour'] = df_clean.index.hour\n",
        "df_clean['minute'] = df_clean.index.minute\n",
        "\n",
        "print(df_clean[['year', 'month', 'day', 'hour', 'minute']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sEQ-qpL5p22"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# CYCLIC ENCODING\n",
        "\n",
        "df_clean.loc[:, 'month_sin'] = np.sin(2 * np.pi * df_clean['month'] / 12.0)\n",
        "df_clean.loc[:, 'month_cos'] = np.cos(2 * np.pi * df_clean['month'] / 12.0)\n",
        "\n",
        "df_clean.loc[:, 'day_sin'] = np.sin(2 * np.pi * df_clean['day'] / 31.0)\n",
        "df_clean.loc[:, 'day_cos'] = np.cos(2 * np.pi * df_clean['day'] / 31.0)\n",
        "\n",
        "df_clean.loc[:, 'hour_sin'] = np.sin(2 * np.pi * df_clean['hour'] / 24.0)\n",
        "df_clean.loc[:, 'hour_cos'] = np.cos(2 * np.pi * df_clean['hour'] / 24.0)\n",
        "\n",
        "df_clean.loc[:, 'minute_sin'] = np.sin(2 * np.pi * df_clean['minute'] / 60.0)\n",
        "df_clean.loc[:, 'minute_cos'] = np.cos(2 * np.pi * df_clean['minute'] / 60.0)\n",
        "\n",
        "\n",
        "# Original time features\n",
        "original_time_cols = ['year', 'month', 'day', 'hour', 'minute']\n",
        "# Cyclical time features\n",
        "cyclical_time_cols = ['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos']\n",
        "\n",
        "\n",
        "# Create Cyclical Features & Drop Originals ---\n",
        "print(\"Dropping original time columns used for cyclical features...\")\n",
        "# Check if columns exist before dropping\n",
        "cols_to_drop_time = [col for col in ['month', 'day', 'hour', 'minute'] if col in df_clean.columns]\n",
        "if cols_to_drop_time:\n",
        "    df_clean = df_clean.drop(columns=cols_to_drop_time)\n",
        "    print(f\"Dropped: {cols_to_drop_time}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4eYntLex3pN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "target = 'InvPDC_kW_Avg'  # dc Inverter Power Output in kW\n",
        "\n",
        "irradiance_features = [\n",
        "    'RefCell1_Wm2_Avg',\n",
        "    'SEWSPOAIrrad_Wm2_Avg',\n",
        "    'Pyra1_Wm2_Avg',\n",
        "    'Pyra2_Wm2_Avg'\n",
        "]\n",
        "\n",
        "temperature_features = [\n",
        "    'AmbTemp_C_Avg',\n",
        "    'SEWSModuleTemp_C_Avg',\n",
        "    'CR1000Temp_C_Avg',\n",
        "    'SEWSAmbientTemp_C_Avg',\n",
        "    'RTD_C_Avg_1',\n",
        "    'RTD_C_Avg_2',\n",
        "    'RTD_C_Avg_3',\n",
        "    'RTD_C_Avg_4',\n",
        "    'RTD_C_Avg_5',\n",
        "    'RTD_C_Avg_6',\n",
        "    'RTD_C_Avg_7',\n",
        "    'RTD_C_Avg_8',\n",
        "    'RTD_C_Avg_9',\n",
        "    'RTD_C_Avg_10'\n",
        "]\n",
        "\n",
        "time_features = [\n",
        "    'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos'\n",
        "]\n",
        "\n",
        "selected_features = irradiance_features + temperature_features + time_features\n",
        "\n",
        "# FINAL DATASET\n",
        "X = df_clean[selected_features]\n",
        "y = df_clean[target]\n",
        "\n",
        "print(\"Selected features:\", X.columns.tolist())\n",
        "print(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXZDLNcJ8RuP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "target_col = 'InvPDC_kW_Avg'\n",
        "\n",
        "irradiance_cols = ['RefCell1_Wm2_Avg',\n",
        "    'SEWSPOAIrrad_Wm2_Avg',\n",
        "    'Pyra1_Wm2_Avg',\n",
        "    'Pyra2_Wm2_Avg']\n",
        "temperature_cols = ['AmbTemp_C_Avg',\n",
        "    'SEWSModuleTemp_C_Avg',\n",
        "    'CR1000Temp_C_Avg',\n",
        "    'SEWSAmbientTemp_C_Avg',\n",
        "    'RTD_C_Avg_1',\n",
        "    'RTD_C_Avg_2',\n",
        "    'RTD_C_Avg_3',\n",
        "    'RTD_C_Avg_4',\n",
        "    'RTD_C_Avg_5',\n",
        "    'RTD_C_Avg_6',\n",
        "    'RTD_C_Avg_7',\n",
        "    'RTD_C_Avg_8',\n",
        "    'RTD_C_Avg_9',\n",
        "    'RTD_C_Avg_10']\n",
        "electric_cols = electrical_features = [\n",
        "    'InvVDVoltage_V_Avg', 'InvIDCin_Avg', 'InvVDCin_Avg', 'InvVPVin_Avg'\n",
        "]\n",
        "\n",
        "# LAG FEATURES\n",
        "lags = [1, 5, 15, 60, 1440]\n",
        "\n",
        "print(\"\\nCreating lag features...\")\n",
        "# Lag target\n",
        "if target_col in df_clean.columns:\n",
        "    for lag in lags:\n",
        "        df_clean[f'{target_col}_lag_{lag}'] = df_clean[target_col].shift(lag)\n",
        "else:\n",
        "    print(f\"Warning: Target column '{target_col}' not found for lagging.\")\n",
        "\n",
        "# Lag key predictors\n",
        "for col in irradiance_cols:\n",
        "    if col in df_clean.columns:\n",
        "        for lag in lags:\n",
        "            df_clean[f'{col}_lag_{lag}'] = df_clean[col].shift(lag)\n",
        "    else:\n",
        "         print(f\"Warning: Irradiance column '{col}' not found for lagging.\")\n",
        "\n",
        "for col in electric_cols:\n",
        "    if col in df_clean.columns:\n",
        "        for lag in lags:\n",
        "            df_clean[f'{col}_lag_{lag}'] = df_clean[col].shift(lag)\n",
        "\n",
        "\n",
        "#ROLLING WINDOW STATISTICS\n",
        "windows = [5, 15, 60]\n",
        "cols_for_rolling = irradiance_cols + temperature_cols\n",
        "\n",
        "print(\"\\nCreating rolling window features...\")\n",
        "for col in cols_for_rolling:\n",
        "    if col in df_clean.columns:\n",
        "        for window in windows:\n",
        "            df_clean[f'{col}_roll_mean_{window}'] = df_clean[col].rolling(window=window, min_periods=2).mean()\n",
        "            df_clean[f'{col}_roll_std_{window}'] = df_clean[col].rolling(window=window, min_periods=2).std()\n",
        "    else:\n",
        "        print(f\"Warning: Column '{col}' for rolling features not found.\")\n",
        "\n",
        "for window in windows:\n",
        "    df_clean[f'{target_col}_roll_mean_{window}'] = df_clean[target_col].rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "\n",
        "\n",
        "# HANDLE NaNs\n",
        "print(f\"\\nDataFrame shape before handling NaNs: {df_clean.shape}\")\n",
        "rows_before = len(df_clean)\n",
        "\n",
        "df_processed = df_clean.dropna()\n",
        "rows_after = len(df_processed)\n",
        "print(f\"DataFrame shape after handling NaNs: {df_processed.shape}\")\n",
        "print(f\"Number of rows dropped due to NaNs: {rows_before - rows_after}\")\n",
        "\n",
        "\n",
        "# FINAL FEATURE LIST\n",
        "\n",
        "feature_cols = irradiance_cols + temperature_cols\n",
        "\n",
        "feature_cols += [col for col in cyclical_time_cols if col in df_processed.columns]\n",
        "\n",
        "if 'year' in df_processed.columns:\n",
        "    feature_cols += ['year']\n",
        "feature_cols += [col for col in df_processed.columns if '_lag_' in col]\n",
        "\n",
        "feature_cols += [col for col in df_processed.columns if '_roll_' in col]\n",
        "\n",
        "\n",
        "feature_cols = [col for col in feature_cols if col != target_col]\n",
        "\n",
        "feature_cols = sorted(list(set(feature_cols)))\n",
        "\n",
        "print(f\"\\nFinal list of {len(feature_cols)} features selected for X:\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5CtsFMu_gJn"
      },
      "outputs": [],
      "source": [
        "print(feature_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ik1x6y_Pp7C"
      },
      "outputs": [],
      "source": [
        "df[target_col].hist(bins=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve91-ow7QQq7"
      },
      "outputs": [],
      "source": [
        "df[target_col] = df[target_col].clip(lower=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG2dW1X29Lz-"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    X = df_processed[feature_cols]\n",
        "    y = df_processed[target_col]\n",
        "except KeyError as e:\n",
        "    print(f\"Error selecting features/target: {e}. Check column names in df_processed.\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"Error: 'df_processed' not defined. Ensure previous steps ran.\")\n",
        "\n",
        "\n",
        "# TIME SERIES DATA SPLIT\n",
        "total_rows = len(df_processed)\n",
        "train_size = int(total_rows * 0.7)\n",
        "val_size = int(total_rows * 0.15)\n",
        "\n",
        "if 'X' in locals() and 'y' in locals():\n",
        "    X_train = X.iloc[:train_size]\n",
        "    y_train = y.iloc[:train_size]\n",
        "    X_val = X.iloc[train_size : train_size + val_size]\n",
        "    y_val = y.iloc[train_size : train_size + val_size]\n",
        "    X_test = X.iloc[train_size + val_size :]\n",
        "    y_test = y.iloc[train_size + val_size :]\n",
        "\n",
        "    print(f\"\\nData Split Shapes:\")\n",
        "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "    print(f\"X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
        "    print(f\"X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n",
        "else:\n",
        "    print(\"\\nSkipping split and scaling due to previous error.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTtC87YiAwRN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# FEATURE SCALING\n",
        "if 'X_train' in locals():\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    print(f\"\\nApplying {type(scaler).__name__} scaling...\")\n",
        "\n",
        "    scaler.fit(X_train)\n",
        "\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
        "    print(\"\\nScaled Training Data Head (as DataFrame):\")\n",
        "    print(X_train_scaled_df.head())\n",
        "\n",
        "    print(\"\\nPreprocessing complete. Ready for model training.\")\n",
        "else:\n",
        "    print(\"\\nScaling skipped as data splitting failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBna2x_ILNIU"
      },
      "outputs": [],
      "source": [
        "import cudf\n",
        "import cuml\n",
        "from cuml.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQRXRbWSLPN4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import cudf\n",
        "\n",
        "\n",
        "X_train.index = X_train.index.tz_localize(None)\n",
        "X_val.index = X_val.index.tz_localize(None)\n",
        "X_test.index = X_test.index.tz_localize(None)\n",
        "\n",
        "\n",
        "X_train_scaled_cudf = cudf.DataFrame.from_pandas(pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index))\n",
        "y_train_cudf = cudf.Series(y_train.values)\n",
        "X_val_scaled_cudf = cudf.DataFrame.from_pandas(pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index))\n",
        "X_test_scaled_cudf = cudf.DataFrame.from_pandas(pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index))\n",
        "y_val_cudf = cudf.Series(y_val.values)\n",
        "y_test_cudf = cudf.Series(y_test.values)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40oRGRJJLR0o"
      },
      "outputs": [],
      "source": [
        "#TESTING ON RANDOM FOREST REGRESSOR\n",
        "rf_model_cuml = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "rf_model_cuml.fit(X_train_scaled_cudf, y_train_cudf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIc1lLJMLTOH"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_pred_val_cudf = rf_model_cuml.predict(X_val_scaled_cudf)\n",
        "y_pred_test_cudf = rf_model_cuml.predict(X_test_scaled_cudf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgpNWuGsLU_L"
      },
      "outputs": [],
      "source": [
        "y_pred_val = y_pred_val_cudf.to_pandas()\n",
        "y_pred_test = y_pred_test_cudf.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1DfBpgMFUYn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "mse_val = mean_squared_error(y_val, y_pred_val)\n",
        "r2_val = r2_score(y_val, y_pred_val)\n",
        "\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Validation MSE: {mse_val}, R-squared: {r2_val}\")\n",
        "print(f\"Test MSE: {mse_test}, R-squared: {r2_test}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpoVXICTNaRK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_val, label='Actual')\n",
        "plt.plot(y_pred_val, label='Predicted')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('InvPDC_kW_Avg')\n",
        "plt.title('Actual vs. Predicted Values (Validation Set)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dehDVtILKXj"
      },
      "outputs": [],
      "source": [
        "pip install --no-build-isolation mamba-ssm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReO2zw9gjaAb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import Adam\n",
        "from torch.nn import MSELoss\n",
        "from mamba_ssm import Mamba\n",
        "from torch.nn import Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhTVQ5K7197_"
      },
      "outputs": [],
      "source": [
        "# PREPARE DATA\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(\"cuda\")\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(\"cuda\")\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(\"cuda\")\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).to(\"cuda\")\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(\"cuda\")\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "# RESHAPE\n",
        "X_train_tensor = X_train_tensor.unsqueeze(1)\n",
        "X_val_tensor = X_val_tensor.unsqueeze(1)\n",
        "X_test_tensor = X_test_tensor.unsqueeze(1)\n",
        "\n",
        "#INITIALIZE\n",
        "input_size = X_train_tensor.shape[2]\n",
        "model0 = Mamba(d_model=input_size, d_state=16, d_conv=4, expand=2).to(\"cuda\")\n",
        "\n",
        "output_layer = Linear(input_size, 1).to(\"cuda\")\n",
        "\n",
        "optimizer = Adam(list(model0.parameters()) + list(output_layer.parameters()), lr=1e-3)\n",
        "loss_fn = MSELoss()\n",
        "num_epochs = 50\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model0.train()\n",
        "    output_layer.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model0(data)\n",
        "        output = output_layer(output.squeeze(1))\n",
        "        loss = loss_fn(output.squeeze(), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJMEvuhv0Vec"
      },
      "outputs": [],
      "source": [
        "\n",
        "model0.eval()\n",
        "output_layer.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_test_tensor = model0(X_test_tensor)\n",
        "    y_pred_test_tensor = output_layer(y_pred_test_tensor.squeeze(1))\n",
        "    y_pred_test_tensor = y_pred_test_tensor.squeeze()\n",
        "y_pred_test = y_pred_test_tensor.cpu().numpy()\n",
        "\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "print(f\"Test MSE: {mse_test}, R-squared: {r2_test}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvOraXgY0PcG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "y_pred_test_tensor = model0(X_test_tensor)\n",
        "y_pred_test_tensor = output_layer(y_pred_test_tensor.squeeze(1))\n",
        "y_pred_test_tensor = y_pred_test_tensor.squeeze()\n",
        "y_pred_test = y_pred_test_tensor.cpu().numpy()\n",
        "\n",
        "y_test = y_test_tensor.cpu().numpy()\n",
        "\n",
        "time_index = X_test_tensor[:, 0, 0].cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(time_index, y_test, label='Actual')\n",
        "plt.plot(time_index, y_pred_test, label='Predicted (Mamba)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('InvPDC_kW_Avg')\n",
        "plt.title('Actual vs. Predicted Values (Mamba Model - Test Set)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
